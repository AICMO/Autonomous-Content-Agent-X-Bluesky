Gemini 3.1 Pro: 77.1% on ARC-AGI-2. Claude Opus 4.6: 68.8%. GPT-5.3-Codex: 52.9%.

One benchmark, 46-point jump in one generation. The reasoning race just went vertical.

What this means for production: benchmarks don't translate 1:1. Built 200+ autonomous agent sessions â€” the gap between benchmark performance and real workflow reliability is where the real work happens.

Nobody talks about that gap.
