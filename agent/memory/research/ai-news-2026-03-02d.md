# AI News — March 2, 2026 (Session #281)
**Created**: Session #281 — 2 new angles (N80-N81)

**Usage**: Queue <15 on both platforms → pick highest-priority angle, apply publishing skill templates.
**Priority**: N81 (Nvidia GTC, secret chip) is most time-sensitive — GTC starts March 16. N80 (MiniMax) is evergreen.

---

## News Angles (Priority Order)

**N80: MiniMax M2.5 — Open-Source AI Matches Claude at 1/20th the Cost**
Hook: "Chinese startup just open-sourced a model that matches Claude Opus on coding benchmarks. Cost: $0.15/task vs $3.00 for Claude. You can run 4 AI agents for an entire year for $10,000 total."
Data:
- MiniMax M2.5 released Feb 11, 2026 on Hugging Face (modified MIT license)
- 230B parameter MoE model — only 10B activated per inference (4% of parameters)
- SWE-Bench Verified: 80.2% (matches Claude Opus 4.6), first open-weight model to beat Claude Sonnet
- BFCL multi-turn function calling: 76.8% — beats Claude Opus by 13 percentage points
- Cost: $0.15/M input, $1.20/M output — vs Claude Opus 4.6 at ~$15/M input, $75/M output (20x cheaper)
- $0.15/task vs $3.00/task for Claude (20x cheaper per task)
- 4 AI agent instances running full year = $10,000 total on M2.5 vs $200,000+ on Claude
- OpenHands ranking: 4th overall (behind Claude Opus 4.6, 4.5, GPT-5.2 Codex)
- Caveats: previous MiniMax models had reward-hacking issues; general reasoning lags closed frontier models; company at $512M losses through Sep 2025
Source: VentureBeat (Feb 11, 2026), WinBuzzer, MiniMax official, OpenHands blog, ThursdAI podcast
Angle: The cost compression story. Not just "Chinese AI is catching up" — the $10K/year for 4 agents calculation is a concrete, shareable number that hits enterprises in the gut. Connects directly to call center AI expertise (cost reduction is THE business driver). Author perspective: "I've been building call center AI for 7 years. The cost curve just broke."

---

**N81: Nvidia's Secret Inference Chip — 90% Cheaper AI at GTC March 16**
Hook: "Nvidia is about to unveil a secret AI chip at GTC March 16. 90% lower cost per token vs Blackwell. OpenAI already signed as lead customer. AI inference is about to get 10x cheaper."
Data:
- Nvidia GTC 2026 conference: March 16-19, San Jose
- Secret inference processor reported by WSJ (March 1) — integrates Groq Inc. technology (Language Processing Units)
- Vera Rubin platform: up to 90% lower inference cost vs Blackwell platform
- Up to 10x lower cost per token for MoE inference vs Blackwell
- 4x fewer GPUs needed for training MoE models
- OpenAI signed as lead customer — wants Nvidia inference chip to power Codex coding tool
- SambaNova SN50 competing: claims 5x peak speed vs B200, 8x cost advantage for agent workloads
- Lead adopters: AWS, Anthropic, Google, Meta, Microsoft, OpenAI, Perplexity, Mistral, CoreWeave
- Nvidia + Groq acquisition (Dec 2025) — LPU architecture enables inference with dramatically lower energy
Source: WSJ (March 1, 2026), SiliconANGLE, NVIDIA Newsroom (Vera Rubin platform), Motley Fool, Digitimes (SambaNova)
Angle: Inference cost is the AI infrastructure story of 2026. 90% cheaper tokens means products that were previously uneconomical become viable. This is not just Nvidia news — it's the unlocking of an entirely new wave of AI applications. Connects to call center AI: real-time AI agents on every call becomes affordable.
TIMING: GTC March 16-19 — post before or during the event for maximum relevance.

---

## Priority Deployment Order
1. **N78** (DeepSeek V4 — MOST URGENT, expected March 3-5, market-moving)
2. **N77** (Anthropic-Pentagon lawsuit — safe framing, time-sensitive)
3. **N81** (Nvidia GTC inference chip — event starts March 16, post by March 14)
4. **N80** (MiniMax M2.5 — strong but increasingly evergreen, cost angle)
5. **N79** (Enterprise AI agent ROI stats — evergreen, anytime)

---

## Context for Session Planning
- X queue at 18, Bluesky queue at 15 when this research was created
- Session #281 created this as non-content work (queues at limit per publishing skill rules)
- Research conducted via web search March 2, 2026
- **CRITICAL**: N78 (DeepSeek V4) could drop March 3 — highest priority when queue allows
- N81 (Nvidia GTC) has hard deadline: March 14-16 window, then event starts
